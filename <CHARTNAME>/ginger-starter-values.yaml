nameOverride: ""
fullnameOverride: ""

name: ""

clusterTier: "" #-- [onco-dev, onco-prod]
deploymentTier: "" #-- [dev, qa, stg, prod]

additionalLabels: {}

deploymentAnnotations: {}

podAnnotations:
  gke-gcsfuse/volumes: 'true'

podLabels:
  k8s.lunit.in/app: "{{ template `fullname` . }}"

createAggregateRoles: false
createClusterRoles: false
clusterRoleRules:
  enabled: false
  rules: [ ]

replicas: 1

runtimeClassName: ""

revisionHistoryLimit: 10

pdb:
  enabled: false
  labels: {}
  annotations: {}
  minAvailable: ""
  maxUnavailable: ""

image:
  repository: nginx
  tag: latest
  imagePullPolicy: IfNotPresent

imagePullSecrets:
  - name: "{{ template `fullname` . }}-dockerconfigjson"

command: [ ]

args: [ ]

resources:
  limits:
    memory: 64Mi
  requests:
    cpu: 10m
    memory: 64Mi

env: [ ]

envFrom:
  - configMapRef:
      name: "{{ template `fullname` . }}-default"
  - secretRef:
      name: "{{ template `fullname` . }}-default"

lifecycle: {}

initContainers: [ ]

extraContainers: [ ]
#  - name: cmp-my-plugin
#    command:
#      - "/var/run/argocd/argocd-cmp-server"
#    image: busybox:latest
#    securityContext:
#      runAsNonRoot: true
#      runAsUser: 999
#    volumeMounts:
#      - mountPath: /var/run/argocd
#        name: var-files

securityContext: {}
#  runAsUser: 1000
#  runAsGroup: 1000
#  fsGroup: 1000

containerSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
#  readOnlyRootFilesystem: false
#  allowPrivilegeEscalation: false
#  seccompProfile:
#    type: RuntimeDefault
#  capabilities:
#    drop:
#    - ALL

readinessProbe:
  enabled: false
  failureThreshold: 30
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1

livenessProbe:
  enabled: true
  failureThreshold: 60
  initialDelaySeconds: 10
  periodSeconds: 5
  successThreshold: 1
  timeoutSeconds: 10

terminationGracePeriodSeconds: 30

nodeSelector:
  cloud.google.com/gke-gpu-driver-version: default
  cloud.google.com/gke-accelerator: nvidia-l4

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

affinity:
  podAntiAffinity:
    type: none
  nodeAffinity:
    type: none

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule

deploymentStrategy: {}
# type: RollingUpdate
# rollingUpdate:
#   maxSurge: 25%
#   maxUnavailable: 25%

priorityClassName: ""

containerPorts:
  - name: http
    containerPort: 8080
    protocol: TCP

service: {}

dualStack: {}

hostNetwork: false

dnsConfig: {}
dnsPolicy: "ClusterFirst"

automountServiceAccountToken: true

serviceAccount:
  create: true
  name: ""
  labels: {}
  automountServiceAccountToken: true
  annotations: {}
#    iam.gke.io/gcp-service-account: ginger@oncology-dev-447022.iam.gserviceaccount.com

################################################################################
# Configs
################################################################################
configs:
  cm:
    create: true
    annotations: {}
    data:
      KEY1: "VALUE1"

  secret:
    create: true
    labels: {}
    annotations: {}
    refreshInterval: 60s
    secretStoreRef:
      name: gcp-in-cluster
    data:
      - secretKey: RABBITMQ_PASSWORD
        remoteRef:
          key: "rabbitmq-onco-{{ .Values.deploymentTier }}-password"
          version: latest
          conversionStrategy: Default
          decodingStrategy: None
          metadataPolicy: None

  extraSecrets:
    - name: dockerconfigjson
      refreshInterval: "60s"
      secretStoreRef:
        name: gcp-in-cluster
      targetTemplate:
        type: kubernetes.io/dockerconfigjson
        data:
          .dockerconfigjson: '{{ .dockerconfigjson | toString }}'
      data:
        - secretKey: dockerconfigjson
          remoteRef:
            key: gke-harbor-docker-config-json
    - name: keda-trigger-auth
      refreshInterval: 60s
      secretStoreRef:
        name: gcp-in-cluster
      data:
        - secretKey: host
          remoteRef:
            key: "keda-rabbitmq-onco-{{ .Values.deploymentTier }}-auth-http"

################################################################################
# Volumes
################################################################################
#-- NOTE
#-- 1. MUST specify .podVolumeMounts[] and .podVolumes[]
#-- 2. key is the name of the pv and pvc
volumes:
  pv:
    shared-vol:
      type: filestore
      filestoreInstance: lunit-common-ssd
      path: one-ai-dev
      size: 1000Gi
      autoCreatePVC: true
    models:
      storageClassName: virt-gcsfuse-rwx
      accessModes:
        - ReadWriteMany
      size: 10Gi
      mountOptions:
        - implicit-dirs
        - file-cache:enable-parallel-downloads:true
        - only-dir=models
        - uid=1000
        - gid=1000
      csi:
        driver: gcsfuse.csi.storage.gke.io
        volumeHandle: one-ai-dev
  pvc:
    models:
      annotations: {}
      storageClassName: virt-gcsfuse-rwx
      accessModes:
        - ReadWriteMany
      volumeName: '{{ template `fullname` . }}-models'
      size: 10Gi

podVolumeMounts:
  - name: shared-vol
    mountPath: /shared-vol
  - name: models
    mountPath: /models
  - name: shmdir
    mountPath: /dev/shm

podVolumes:
  - name: shared-vol
    persistentVolumeClaim:
      claimName: '{{ template `fullname` . }}-shared-vol'
  - name: models
    persistentVolumeClaim:
      claimName: '{{ template `fullname` . }}-models'
  - name: shmdir
    emptyDir:
      sizeLimit: 4Gi

################################################################################
# Istio
################################################################################
istio:
  create: false

################################################################################
# AutoScaling
################################################################################
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 50
  targetMemoryUtilizationPercentage: 50
  behavior: {}
    # scaleDown:
    #  stabilizationWindowSeconds: 300
    #  policies:
    #   - type: Pods
    #     value: 1
  #     periodSeconds: 180
  # scaleUp:
  #   stabilizationWindowSeconds: 300
  #   policies:
  #   - type: Pods
  #     value: 2
  #     periodSeconds: 60
  metrics: [ ]

kedaAutoscaling:
  enabled: true
  annotations: {}
  minReplicaCount: 0
  maxReplicaCount: 10
  pollingInterval: 3
  cooldownPeriod: 5
  triggerAuthentication:
    secretTargetRef:
      - parameter: host
        name: '{{ template `fullname` . }}-keda-trigger-auth'
        key: host
#  cooldownPeriod: 300
#  idleReplicaCount: 0
#  initialCooldownPeriod: 0
#  scaleTargetRef:
#    apiVersion: apps/v1
#    kind: Deployment
#    envSourceContainerName: ""
#  fallback:
#    failureThreshold: 3
#    replicas: 1
#  advanced:
#    restoreToOriginalReplicaCount: false
#    horizontalPodAutoscalerConfig:
#      behavior:
#        scaleDown:
#          stabilizationWindowSeconds: 300
#          policies:
#            - type: Percent
#              value: 100
#              periodSeconds: 75
#  triggers:
#    - type: rabbitmq
#      metadata:
#        protocol: auto
#        mode: QueueLength
#        value: "100.50"
#        activationValue: "10.5"
#        queueName: testqueue
#        vhostName: /
#  triggerAuthentication:
#    secretTargetRef:
#      - parameter: host
#        name: "{{ template `fullname` . }}-keda-rabbitmq-auth" #-- follow external secret name
#        key: host

################################################################################
# OpenTelemetry
################################################################################
openTelemetry:
  enabled: false

  instrumentation:
    annotations: {}
    exporter:
      endpoint: "http://{{ template `fullname` . }}-collector:4317"
    propagators:
      - tracecontext
      - baggage
    sampler:
      type: parentbased_traceidratio
      argument: "0.25"
    python: {}
    go: {}
  #      env:
  #        - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #          value: "http://{{ template `fullname` . }}-collector:4318"

  collector:
    labels: {}
    annotations: {}
    mode: deployment
    replicas: 1
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 10m
        memory: 128Mi
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
      processors:
        probabilistic_sampler:
          sampling_percentage: 20
        filter:
          error_mode: ignore
          traces:
            span:
              - 'attributes["http.target"] == "/health"'
        memory_limiter:
          check_interval: 1s
          limit_percentage: 75
          spike_limit_percentage: 15
        batch:
          send_batch_size: 10000
          send_batch_max_size: 20000
          timeout: 10s
      exporters:
        otlphttp/tempo:
          endpoint: "http://tempo-mgmt-distributor.observability-system:4318"
          headers:
            x-scope-orgid: "{{ .Values.clusterTier }}"
      service:
        pipelines:
          traces:
            receivers: [ otlp ]
            processors: [ memory_limiter, filter, probabilistic_sampler, batch ]
            exporters: [ otlphttp/tempo ]

################################################################################
# Batch
################################################################################
batch:
  job:
    init-queue:
      annotations:
        helm.sh/hook: post-install,post-upgrade
      completions: 1
      parallelism: 1
      activeDeadlineSeconds: 5
      ttlSecondsAfterFinished: 5
      backoffLimit: 10
      jobTemplate:
        metadata:
          labels:
            sidecar.istio.io/inject: 'false'
        spec:
          containers:
            - name: init-queue
              image: quay.io/curl/curl:latest
              imagePullPolicy: IfNotPresent
              command:
                - /bin/sh
                - -c
                - |
                  curl -v -XPUT \
                    -H "Content-Type: application/json" \
                    -d '{"durable": true, "arguments": {"x-max-priority": 255}}' \
                    "http://${RABBITMQ_USERNAME}:${RABBITMQ_PASSWORD}@${RABBITMQ_HOST}:15672/api/queues/%2f/${RPC_QUEUE_NAME}" \
                  && curl -v -XPOST \
                    -H "Content-Type: application/json" \
                    -d "{\"routing_key\": \"${RPC_ROUTING_KEY}\"}" \
                    "http://${RABBITMQ_USERNAME}:${RABBITMQ_PASSWORD}@${RABBITMQ_HOST}:15672/api/bindings/%2f/e/tasks/q/${RPC_QUEUE_NAME}"
              envFrom:
                - configMapRef:
                    name: '{{ template `fullname` . }}-default'
                - secretRef:
                    name: '{{ template `fullname` . }}-default'
          restartPolicy: OnFailure

  cronjob: {}
#    job-delete:
#      schedule: "* * * * *"
#      timeZone: "UTC"
#      startingDeadlineSeconds: 180
#      concurrencyPolicy: Allow
#      successfulJobsHistoryLimit: 3
#      failedJobsHistoryLimit: 0
#      jobTemplate:
#        spec:
#          template:
#            metadata:
#              labels:
#                sidecar.istio.io/inject: "false"
#            spec:
#              containers:
#                - name: hello
#                  image: busybox:1.28
#                  imagePullPolicy: IfNotPresent
#                  command:
#                    - /bin/sh
#                    - -c
#                    - date; echo Hello from the Kubernetes cluster
#              restartPolicy: OnFailure
